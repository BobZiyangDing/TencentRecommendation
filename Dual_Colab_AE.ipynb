{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Collaborative Filtering Autoencoder Metric Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def sparseEmbed(df, name, num, colIdx):\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)] \n",
    "    Emptydf = pd.DataFrame()\n",
    "    Emptydf[embedName] = df[name].str.split('|',expand=True)\n",
    "    values = np.unique(Emptydf[embedName].values)\n",
    "    \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in values:\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "    \n",
    "    \n",
    "    appendValue = np.zeros([Emptydf.values.shape[0], len(values)])\n",
    "    for i in range(Emptydf.values.shape[0]):\n",
    "        for j in range(num):\n",
    "            key = Emptydf.values[i][j]\n",
    "            if key in dic:\n",
    "                appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def toDummy(df, name, colIdx):\n",
    "    num = len(np.unique(df[name].values.astype(str)))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def genderDummy(df, name, colIdx):\n",
    "    pool = set()\n",
    "    num = len(np.unique(df[name].values))-1\n",
    "    for i in df[name].values:\n",
    "        pool.add(str(i))\n",
    "    num = len(list(pool))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and transforming to categorical binary input data form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with user_7_hero\n",
      "finished with user_30_hero\n",
      "finished with user_7_keyword\n",
      "finished with user_7_author\n",
      "finished with item_author\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_gender_0</th>\n",
       "      <th>user_gender_1</th>\n",
       "      <th>user_gender_2</th>\n",
       "      <th>user_gender_3</th>\n",
       "      <th>user_7_hero_0</th>\n",
       "      <th>user_7_hero_1</th>\n",
       "      <th>user_7_hero_2</th>\n",
       "      <th>user_7_hero_3</th>\n",
       "      <th>user_7_hero_4</th>\n",
       "      <th>...</th>\n",
       "      <th>item_author_519</th>\n",
       "      <th>item_author_520</th>\n",
       "      <th>item_author_521</th>\n",
       "      <th>item_author_522</th>\n",
       "      <th>item_author_523</th>\n",
       "      <th>item_author_524</th>\n",
       "      <th>item_avgTime</th>\n",
       "      <th>item_numReader</th>\n",
       "      <th>item_numTime</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322051</th>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>0.038687</td>\n",
       "      <td>0.005559</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309142</th>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045477</td>\n",
       "      <td>0.062402</td>\n",
       "      <td>0.019748</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39702</th>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.068731</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232882</th>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055037</td>\n",
       "      <td>0.250331</td>\n",
       "      <td>0.095872</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30629</th>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049202</td>\n",
       "      <td>0.628562</td>\n",
       "      <td>0.215205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_age  user_gender_0  user_gender_1  user_gender_2  user_gender_3  \\\n",
       "322051  0.293333            0.0            1.0            0.0            0.0   \n",
       "309142  0.213333            0.0            1.0            0.0            0.0   \n",
       "39702   0.373333            0.0            1.0            0.0            0.0   \n",
       "232882  0.293333            0.0            1.0            0.0            0.0   \n",
       "30629   0.280000            0.0            1.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_0  user_7_hero_1  user_7_hero_2  user_7_hero_3  \\\n",
       "322051            0.0            0.0            0.0            0.0   \n",
       "309142            0.0            0.0            0.0            0.0   \n",
       "39702             0.0            0.0            0.0            0.0   \n",
       "232882            0.0            0.0            0.0            0.0   \n",
       "30629             0.0            0.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_4  ...  item_author_519  item_author_520  item_author_521  \\\n",
       "322051            0.0  ...              0.0              0.0              0.0   \n",
       "309142            1.0  ...              0.0              0.0              0.0   \n",
       "39702             0.0  ...              0.0              0.0              0.0   \n",
       "232882            0.0  ...              0.0              0.0              0.0   \n",
       "30629             0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "        item_author_522  item_author_523  item_author_524  item_avgTime  \\\n",
       "322051              0.0              0.0              0.0      0.020649   \n",
       "309142              0.0              0.0              0.0      0.045477   \n",
       "39702               0.0              0.0              0.0      0.022700   \n",
       "232882              0.0              0.0              0.0      0.055037   \n",
       "30629               0.0              0.0              0.0      0.049202   \n",
       "\n",
       "        item_numReader  item_numTime  label  \n",
       "322051        0.038687      0.005559    0.0  \n",
       "309142        0.062402      0.019748    0.0  \n",
       "39702         0.068731      0.010857    1.0  \n",
       "232882        0.250331      0.095872    0.0  \n",
       "30629         0.628562      0.215205    0.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = [\"user_age\", \"user_gender\", \"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_rate\", \"item_keyword\", \"item_author\", \"item_avgTime\", \"item_numReader\", \"item_numTime\", \"label\"]\n",
    "raw = pd.read_csv(\"./thing.txt\", names=head, sep=\",\", index_col = False)\n",
    "\n",
    "colIdx = raw.columns.values.tolist().index(\"user_gender\")\n",
    "raw = genderDummy(raw, \"user_gender\", colIdx)\n",
    "colIdx = raw.columns.values.tolist().index(\"item_keyword\")\n",
    "raw = toDummy(raw, \"item_keyword\", colIdx)\n",
    "\n",
    "numDic = {\"user_gender\": 1, \"user_7_hero\": 5, \"user_30_hero\": 5, \"user_7_keyword\": 3, \"user_7_author\": 3, \"item_keyword\": 1, \"item_author\": 3}\n",
    "for i in [\"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_author\"]:\n",
    "    colIdx = raw.columns.values.tolist().index(i)\n",
    "    raw = sparseEmbed(raw, i, numDic[i], colIdx)\n",
    "    print(\"finished with\", i)\n",
    "\n",
    "# normalize numerical features into interval [0, 1]\n",
    "for i in [\"user_age\", \"item_rate\", \"item_avgTime\", \"item_numReader\", \"item_numTime\"]:\n",
    "    r = raw[i].values.astype(float)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    raw_normalized = pd.DataFrame(x_scaled)\n",
    "    raw[i] = raw_normalized\n",
    "\n",
    "raw = raw.sample(200000)\n",
    "    \n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: (3000, 2118) | validate data size: (1000, 2118) | testing data size: (1000, 2118)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 16:12:18.485413 14308 deprecation.py:323] From C:\\Users\\ziyangding\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py:809: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.train' has no attribute 'RMXPropOptimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-05dbae32b857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    215\u001b[0m               biases[\"decoder_bgh1\"], biases[\"decoder_bgh2\"], biases[\"decoder_bgh3\"], biases[\"decoder_b_g_to_g_out\"]]\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m \u001b[0moptimizer_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMXPropOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_var_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[0moptimizer_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMXPropOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg_var_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_dw_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accessing local variables before they are created.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     if (self._dw_warning_count < _PER_MODULE_WARNING_LIMIT and\n\u001b[0;32m    108\u001b[0m         name not in self._dw_deprecated_printed):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.train' has no attribute 'RMXPropOptimizer'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = raw.sample(5000)\n",
    "\n",
    "# Splitting dataframe into train, validation, and testing\n",
    "dataY = data['label'].values\n",
    "dataX = data.drop(columns = 'label').values\n",
    "\n",
    "\n",
    "X, Xtest, Y, Ytest = train_test_split(dataX, dataY, test_size = 0.2, random_state = 42)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(\"training data size: {} | validate data size: {} | testing data size: {}\".format(str(Xtrain.shape), str(Xval.shape), str(Xtest.shape)))\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.000001\n",
    "batch_size = 256\n",
    "epochs = 800\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = Xtrain.shape[1]\n",
    "num_input_p = data.columns.values.tolist().index(\"item_rate\") # number of all user input columns, the last column ends before the start of \"item_rate\" column\n",
    "num_input_g = data.columns.values.shape[0] - num_input_p - 1   # number of all item input columns, = all column -user -label\n",
    "\n",
    "a = 1\n",
    "num_encode_1 = int(256 *a)\n",
    "num_encode_2 = int(128 *a)\n",
    "num_encode_3 = int(64 *a)\n",
    "\n",
    "\n",
    "num_neck = 5\n",
    "\n",
    "num_decode_1 = num_encode_3\n",
    "num_decode_2 = num_encode_2\n",
    "num_decode_3 = num_encode_1\n",
    "\n",
    "num_output_to_p = num_input_p\n",
    "num_output_to_g = num_input_g\n",
    "\n",
    "#del raw\n",
    "\n",
    "\n",
    "# balance weight coefficient [0,1], the bigger the mode focused onto neck distance\n",
    "alpha = 0.5\n",
    "\n",
    "# regularization\n",
    "beta = 0.0005\n",
    "\n",
    "# collaborative autoencoder input tensor1\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "label = tf.placeholder(\"float\", [None])\n",
    "threshold = tf.Variable(0.0)\n",
    "\n",
    "weights = {\n",
    "    'encoder_ph1': tf.Variable(tf.random_normal([num_input_p , num_encode_1])),\n",
    "    'encoder_gh1': tf.Variable(tf.random_normal([num_input_g , num_encode_1])),\n",
    "    'encoder_ph2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_gh2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_ph3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),\n",
    "    'encoder_gh3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),    \n",
    "\n",
    "\n",
    "    'encoder_pneck': tf.Variable(tf.random_normal([num_encode_3 , num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_gneck': tf.Variable(tf.random_normal([num_encode_3 , num_neck])), ## METRIC SPACE OF GOODS\n",
    "\n",
    "\n",
    "    'decoder_ph1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_gh1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_ph2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    'decoder_gh2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    'decoder_ph3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),\n",
    "    'decoder_gh3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),    \n",
    "\n",
    "\n",
    "    'decoder_p_to_p_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_p])),\n",
    "    'decoder_g_to_g_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_g]))\n",
    "}\n",
    "\n",
    "biases = {  \n",
    "    'encoder_bph1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bgh1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bph2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bgh2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bph3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "    'encoder_bgh3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "\n",
    "\n",
    "    'encoder_bpneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_bgneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF GOODS\n",
    "\n",
    "\n",
    "    'decoder_bph1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bgh1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bph2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    'decoder_bgh2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    'decoder_bph3': tf.Variable(tf.random_normal([num_decode_3])),\n",
    "    'decoder_bgh3': tf.Variable(tf.random_normal([num_decode_3])),    \n",
    "\n",
    "\n",
    "    'decoder_b_p_to_p_out': tf.Variable(tf.random_normal([num_output_to_p])),\n",
    "    'decoder_b_g_to_g_out': tf.Variable(tf.random_normal([num_output_to_g]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "\n",
    "    ## Person encoder:\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(x[:, :1376], weights['encoder_ph1']), biases['encoder_bph1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['encoder_ph2']), biases['encoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['encoder_ph3']), biases['encoder_bph3']))\n",
    "\n",
    "    layer_p_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_3, weights['encoder_pneck']), biases['encoder_bpneck']))\n",
    "\n",
    "    ## Good encoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(x[:, 1376:], weights['encoder_gh1']), biases['encoder_bgh1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['encoder_gh2']), biases['encoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['encoder_gh3']), biases['encoder_bgh3']))\n",
    "\n",
    "    layer_g_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_3, weights['encoder_gneck']), biases['encoder_bgneck']))\n",
    "\n",
    "\n",
    "    return layer_p_neck, layer_g_neck\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(p_neck, g_neck):\n",
    "\n",
    "    ## Good to Person decoder\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(p_neck, weights['decoder_ph1']), biases['decoder_bph1']))\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['decoder_ph2']), biases['decoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['decoder_ph3']), biases['decoder_bph3']))\n",
    "\n",
    "    layer_p_to_p_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_3, weights['decoder_p_to_p_out']), biases['decoder_b_p_to_p_out']))\n",
    "\n",
    "\n",
    "\n",
    "    ## Person to Good decoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(g_neck, weights['decoder_gh1']), biases['decoder_bgh1']))\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['decoder_gh2']), biases['decoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['decoder_gh3']), biases['decoder_bgh3']))\n",
    "\n",
    "    layer_g_to_g_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_3, weights['decoder_g_to_g_out']), biases['decoder_b_g_to_g_out']))\n",
    "\n",
    "    result = tf.concat([layer_p_to_p_out, layer_g_to_g_out], axis = 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_p, encoder_g = encoder(X)\n",
    "decoder_out = decoder(encoder_p, encoder_g)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_out\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "def getl2loss(dic):\n",
    "    l2 = 0\n",
    "    for i in dic.keys():\n",
    "        l2 += tf.nn.l2_loss(dic[i])\n",
    "    return l2\n",
    "\n",
    "sign = 2*label-1\n",
    "\n",
    "\n",
    "# calculate l2 distance\n",
    "neck_distance_l2 = tf.reshape(tf.norm(encoder_p-encoder_g, axis = 1), [-1])\n",
    "signed_distance_l2 = tf.multiply(neck_distance_l2, sign)\n",
    "\n",
    "\n",
    "\n",
    "# calculate l infinity distance\n",
    "neck_distance_linf = tf.reshape(tf.norm(encoder_p - encoder_g, axis=1, ord = np.infty), [-1])\n",
    "signed_distance_linf = tf.multiply(neck_distance_linf, sign)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4 different losses\n",
    "loss_neck_distance = tf.reduce_mean(tf.maximum(0.0, 0.6*threshold+tf.multiply(sign, signed_distance_l2-threshold)))\n",
    "\n",
    "\n",
    "\n",
    "#signed_centered_distance = -(neck_distance - tf.reduce_mean(neck_distance))\n",
    "# loss_neck_distance = tf.losses.hinge_loss(label, tf.multiply(sign, signed_distance_linf-threshold))\n",
    "loss_pred_distance = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "loss_weights = tf.reduce_sum(getl2loss(weights))\n",
    "loss_bias = tf.reduce_sum(getl2loss(biases))\n",
    "\n",
    "loss = alpha * loss_neck_distance + (1-alpha) * loss_pred_distance + beta * (loss_weights + loss_bias)\n",
    "\n",
    "\n",
    "# Calculating AUC: \n",
    "# NOTE: because threshold is always half of maximum distance, \n",
    "# 2*threshold is maximum distance, normalize to scale of 1\n",
    "# then use 1 to minus will yield a inversion that matches distance proximity property\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auc, _ = tf.metrics.auc(label, tf.cast(tf.less_equal(neck_distance_linf, threshold,), tf.int32  ) )\n",
    "\n",
    "\n",
    "# Define Optimizer\n",
    "p_var_list = [weights[\"encoder_ph1\"], weights[\"encoder_ph2\"], weights[\"encoder_ph3\"], weights[\"encoder_pneck\"], \n",
    "              weights[\"decoder_ph1\"], weights[\"decoder_ph2\"], weights[\"decoder_ph3\"], weights[\"decoder_p_to_p_out\"],\n",
    "              biases[\"encoder_bph1\"], biases[\"encoder_bph2\"], biases[\"encoder_bph3\"], biases[\"encoder_bpneck\"], \n",
    "              biases[\"decoder_bph1\"], biases[\"decoder_bph2\"], biases[\"decoder_bph3\"], biases[\"decoder_b_p_to_p_out\"]]\n",
    "\n",
    "g_var_list = [weights[\"encoder_gh1\"], weights[\"encoder_gh2\"], weights[\"encoder_gh3\"], weights[\"encoder_gneck\"], \n",
    "              weights[\"decoder_gh1\"], weights[\"decoder_gh2\"], weights[\"decoder_gh3\"], weights[\"decoder_g_to_g_out\"],\n",
    "              biases[\"encoder_bgh1\"], biases[\"encoder_bgh2\"], biases[\"encoder_bgh3\"], biases[\"encoder_bgneck\"], \n",
    "              biases[\"decoder_bgh1\"], biases[\"decoder_bgh2\"], biases[\"decoder_bgh3\"], biases[\"decoder_b_g_to_g_out\"]]\n",
    "\n",
    "optimizer_p = tf.train.RMXPropOptimizer(learning_rate).minimize(loss, var_list = p_var_list)\n",
    "optimizer_g = tf.train.RMXPropOptimizer(learning_rate).minimize(loss, var_list = g_var_list)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    sess.run(local_init)\n",
    "\n",
    "\n",
    "    num_train_batches = int(Xtrain.shape[0] / batch_size)\n",
    "    Xtrain = np.array_split(Xtrain, num_train_batches)\n",
    "    Ytrain = np.array_split(Ytrain, num_train_batches)\n",
    "\n",
    "    for i in range(len(Ytrain)):\n",
    "        Ytrain[i] = np.reshape(Ytrain[i], [-1,1])\n",
    "    Yval = np.reshape(Yval, [-1,1])\n",
    "\n",
    "\n",
    "\n",
    "    # Training with validating\n",
    "    true_false_break = math.sqrt(num_neck)/2\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_train_cost = 0\n",
    "        avg_train_neck_cost = 0\n",
    "        avg_train_pred_cost = 0\n",
    "        avg_train_auc =0\n",
    "        for batch in range(len(Xtrain)):\n",
    "\n",
    "            # optimize the person side\n",
    "            _, l, neck_loss, pred_loss, neck_dis_train, train_auc = sess.run([optimizer_p, loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: np.reshape(Ytrain[batch], [-1]), threshold: true_false_break})\n",
    "           # optimize the goods side\n",
    "            _, l, neck_loss, pred_loss, neck_dis_train, train_auc = sess.run([optimizer_g, loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: np.reshape(Ytrain[batch], [-1]), threshold: true_false_break})\n",
    "\n",
    "            avg_train_cost += l\n",
    "            avg_train_neck_cost += neck_loss\n",
    "            avg_train_pred_cost += pred_loss\n",
    "            avg_train_auc += train_auc\n",
    "\n",
    "            \n",
    "        avg_train_cost /= num_train_batches\n",
    "        avg_train_neck_cost /= num_train_batches\n",
    "        avg_train_pred_cost /= num_train_batches\n",
    "        avg_train_auc /= num_train_batches \n",
    "            \n",
    "\n",
    "        # Validate once an epoch ends\n",
    "        val_cost, val_neck_cost, val_pred_loss, neck_dis_val, val_auc = sess.run([loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xval, label: np.reshape(Yval, [-1]), threshold: true_false_break})\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            # look at the training metric space\n",
    "            signed_train = np.multiply(neck_dis_train, 2*Ytrain[batch].flatten()-1)\n",
    "            pos = []\n",
    "            neg = []\n",
    "            for dist in signed_train:\n",
    "                if dist >= 0:\n",
    "                    pos.append(dist)\n",
    "                else:\n",
    "                    neg.append(dist)\n",
    "\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            neg = np.array(neg)\n",
    "\n",
    "            a = np.mean(pos)\n",
    "            b = np.mean(neg)\n",
    "\n",
    "\n",
    "            # look at the validation metric space\n",
    "            signed_val = np.multiply(neck_dis_val.flatten(), 2*Yval.flatten()-1)\n",
    "            pos_val = []\n",
    "            neg_val = []\n",
    "            for dist_val in signed_val:\n",
    "                if dist_val >= 0:\n",
    "                    pos_val.append(dist_val)\n",
    "                else:\n",
    "                    neg_val.append(dist_val)\n",
    "\n",
    "            pos_val = np.array(pos_val)\n",
    "            neg_val = np.array(neg_val)\n",
    "\n",
    "            a_val = np.mean(pos_val)\n",
    "            b_val = np.mean(neg_val)\n",
    "\n",
    "\n",
    "            if i >= 2000:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos_val, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg_val, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            print(\"Epoch: {:>3} | Train Loss: {:+8.2f} | Val Loss: {:+8.3f} | Train Neck: {:+8.3f} | Val Neck: {:+8.3f} | Train Recon: {:+8.3f} | Val Recon: {:+8.3f}\"\n",
    "                  .format( i + 1,    avg_train_cost,         val_cost,        avg_train_neck_cost,      val_neck_cost,       avg_train_pred_cost,    val_pred_loss))\n",
    "            print(\"---------- | Train AUC: {:+8.3f} | Val AUC: {:+8.3f} | TF Break: {:06.3f} | mean pos dist: {:06.3f} | mean neg dist {:06.3f} \"\n",
    "                  .format(avg_train_auc,val_auc,   true_false_break, a, -b))\n",
    "            print()\n",
    "\n",
    "    Ytest = np.reshape(Ytest, [-1,1])\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    test_cost, test_neck_cost, pred_test = sess.run([loss, loss_neck_distance, neck_pred],  # {loss:test_cost, loss_neck_distance:test_neck_cost, neck_pred:pred_test}\n",
    "                                                   feed_dict={X: Xtest, label: Ytest, training: False, pos_neg_break: true_false_break})\n",
    "    test_auc = roc_auc_score(Ytest, pred_test)  # getting testing auc score\n",
    "    print(\"Test Loss: {:02.5f} | Neck Loss: {:02.5f} | AUC: {:02.5f}\".format(test_cost, test_neck_cost, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Subtract, Lambda\n",
    "import keras.backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "\n",
    "batch = 512\n",
    "\n",
    "data = raw.sample(5000)\n",
    "\n",
    "# Splitting dataframe into train, validation, and testing\n",
    "dataY = data['label'].values\n",
    "dataX = data.drop(columns = 'label').values\n",
    "\n",
    "\n",
    "X, Xtest, Y, Ytest = train_test_split(dataX, dataY, test_size = 0.2, random_state = 42)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "\n",
    "break_index = data.columns.values.tolist().index(\"item_rate\") # first item index-1 is the break index\n",
    "length_total = data.values.shape[1]\n",
    "length_p = break_index # index of last user feature into length of the user feature\n",
    "length_g = length_total-length_p-1\n",
    "\n",
    "\n",
    "def pgSplit(data, idx):\n",
    "    data_p = data[:, :idx]\n",
    "    data_g = data[:, idx:]\n",
    "    return data_p, data_g\n",
    "\n",
    "Xtrain_p, Xtrain_g = pgSplit(Xtrain, break_index)\n",
    "Xval_p, Xval_g = pgSplit(Xval, break_index)\n",
    "Xtest_p, Xtest_g = pgSplit(Xtest, break_index)\n",
    "\n",
    "a = 1\n",
    "num_encode_1 = int(256 *a)\n",
    "num_encode_2 = int(128 *a)\n",
    "num_encode_3 = int(64 *a)\n",
    "num_neck = 100\n",
    "num_decode_1 = num_encode_3\n",
    "num_decode_2 = num_encode_2\n",
    "num_decode_3 = num_encode_1\n",
    "num_output_to_p = length_p\n",
    "num_output_to_g = length_g\n",
    "\n",
    "threshold = 0.5 * math.sqrt(num_neck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = Input(shape=(1,))\n",
    "\n",
    "## person autoencoder\n",
    "main_p_input = Input(shape=(length_p,))\n",
    "encode_p_1 = Dense(num_encode_1, activation='relu')(main_p_input)\n",
    "encode_dropout_p_1 = Dropout(0.1)(encode_p_1)\n",
    "encode_p_2 = Dense(num_encode_2, activation='relu')(encode_dropout_p_1)\n",
    "encode_dropout_p_2 = Dropout(0.1)(encode_p_2)\n",
    "encode_p_3 = Dense(num_encode_3, activation='relu')(encode_dropout_p_2)\n",
    "encode_dropout_p_3 = Dropout(0.1)(encode_p_3)\n",
    "encode_p_neck = Dense(num_neck, activation= 'sigmoid')(encode_dropout_p_3) ###\n",
    "decode_p_1 = Dense(num_decode_1, activation='relu')(encode_p_neck)\n",
    "decode_dropout_p_1 = Dropout(0.1)(decode_p_1)\n",
    "decode_p_2 = Dense(num_decode_2, activation='relu')(decode_dropout_p_1)\n",
    "decode_dropout_p_2 = Dropout(0.1)(decode_p_2)\n",
    "decode_p_3 = Dense(num_decode_3, activation='relu')(decode_dropout_p_2)\n",
    "decode_dropout_p_3 = Dropout(0.1)(decode_p_3)\n",
    "output_p_out = Dense(num_output_to_p, activation= 'sigmoid', name = \"p_out\")(decode_dropout_p_3)\n",
    "\n",
    "## goods autoencoder\n",
    "main_g_input = Input(shape=(length_g,))\n",
    "encode_g_1 = Dense(num_encode_1, activation='relu')(main_g_input)\n",
    "encode_dropout_g_1 = Dropout(0.1)(encode_g_1)\n",
    "encode_g_2 = Dense(num_encode_2, activation='relu')(encode_dropout_g_1)\n",
    "encode_dropout_g_2 = Dropout(0.1)(encode_g_2)\n",
    "encode_g_3 = Dense(num_encode_3, activation='relu')(encode_dropout_g_2)\n",
    "encode_dropout_g_3 = Dropout(0.1)(encode_g_3)\n",
    "encode_g_neck = Dense(num_neck, activation= 'sigmoid')(encode_dropout_g_3) ###\n",
    "decode_g_1 = Dense(num_decode_1, activation='relu')(encode_g_neck)\n",
    "decode_dropout_g_1 = Dropout(0.1)(decode_g_1)\n",
    "decode_g_2 = Dense(num_decode_2, activation='relu')(decode_dropout_g_1)\n",
    "decode_dropout_g_2 = Dropout(0.1)(decode_g_2)\n",
    "decode_g_3 = Dense(num_decode_3, activation='relu')(decode_dropout_g_2)\n",
    "decode_dropout_g_3 = Dropout(0.1)(decode_g_3)\n",
    "output_g_out = Dense(num_output_to_g, activation= 'sigmoid', name = \"g_out\")(decode_dropout_g_3)\n",
    "\n",
    "y_recon = tf.concat((output_p_out, output_g_out),1)\n",
    "y_truth = tf.concat((main_p_input, main_g_input),1)\n",
    "\n",
    "def AUC(y_true, label):\n",
    "    auc = tf.metrics.auc(y_true,label)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "def getOutput(X):\n",
    "    distance = tf.norm(X, axis = 1)\n",
    "    output = K.cast(tf.less_equal(distance, threshold), tf.float32)\n",
    "    return output\n",
    "\n",
    "distance = Subtract()([encode_p_neck, encode_g_neck])\n",
    "output = Lambda(getOutput)(distance)\n",
    "\n",
    "\n",
    "# Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer\n",
    "def reconstructionLoss(y_recon, y_truth):\n",
    "    return mean_squared_error(y_recon, y_truth)\n",
    "\n",
    "def covarianceLoss(neck_p, neck_g):\n",
    "    X = tf.concat((neck_p, neck_g), 0)\n",
    "    n_rows = tf.cast(tf.shape(X)[0], tf.float32)\n",
    "    X = X - (tf.reduce_mean(X, axis=0))\n",
    "    cov = tf.matmul(X, X, transpose_a=True) / n_rows\n",
    "    return tf.reduce_sum(tf.matrix_set_diag(cov, tf.zeros(num_neck, tf.float32)))\n",
    "\n",
    "def distanceLoss(neck_p, neck_g, label):\n",
    "    distance = tf.norm(neck_p - neck_g, axis = 1)\n",
    "    sign = 2*label-1\n",
    "    return tf.reduce_mean(tf.maximum(0.0, 0.6*threshold+tf.multiply(sign, distance-threshold)))\n",
    "\n",
    "losses = {\"reconstruction loss\": reconstructionLoss\n",
    "          \"covariance loss\": covarianceLoss\n",
    "          \"distance loss\": distanceLoss}\n",
    "\n",
    "weights = {\"reconstruction loss\": 1\n",
    "          \"covariance loss\": 1\n",
    "          \"distance loss\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-40d56b58f225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmain_p_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_g_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rmsprop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAUC\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXtrain_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mXval_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model(inputs= [main_p_input, main_g_input, label], outputs = [output,  ,y_recon])\n",
    "model.compile(optimizer='rmsprop', loss=losses, loss_weights=weights , metrics=[AUC])\n",
    "model.fit([Xtrain_p, Xtrain_g, Ytrain], Ytrain, validation_data=([Xval_p, Xval_g, Yval], Yval), epochs=20, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
