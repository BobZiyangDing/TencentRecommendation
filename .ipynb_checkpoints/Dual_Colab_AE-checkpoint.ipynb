{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Collaborative Filtering Autoencoder Metric Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def sparseEmbed(df, name, num, colIdx):\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)] \n",
    "    Emptydf = pd.DataFrame()\n",
    "    Emptydf[embedName] = df[name].str.split('|',expand=True)\n",
    "    values = np.unique(Emptydf[embedName].values)\n",
    "    \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in values:\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "    \n",
    "    \n",
    "    appendValue = np.zeros([Emptydf.values.shape[0], len(values)])\n",
    "    for i in range(Emptydf.values.shape[0]):\n",
    "        for j in range(num):\n",
    "            key = Emptydf.values[i][j]\n",
    "            if key in dic:\n",
    "                appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def toDummy(df, name, colIdx):\n",
    "    num = len(np.unique(df[name].values.astype(str)))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def genderDummy(df, name, colIdx):\n",
    "    pool = set()\n",
    "    num = len(np.unique(df[name].values))-1\n",
    "    for i in df[name].values:\n",
    "        pool.add(str(i))\n",
    "    num = len(list(pool))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and transforming to categorical binary input data form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with user_7_hero\n",
      "finished with user_30_hero\n",
      "finished with user_7_keyword\n",
      "finished with user_7_author\n",
      "finished with item_author\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_gender_0</th>\n",
       "      <th>user_gender_1</th>\n",
       "      <th>user_gender_2</th>\n",
       "      <th>user_gender_3</th>\n",
       "      <th>user_7_hero_0</th>\n",
       "      <th>user_7_hero_1</th>\n",
       "      <th>user_7_hero_2</th>\n",
       "      <th>user_7_hero_3</th>\n",
       "      <th>user_7_hero_4</th>\n",
       "      <th>...</th>\n",
       "      <th>item_author_519</th>\n",
       "      <th>item_author_520</th>\n",
       "      <th>item_author_521</th>\n",
       "      <th>item_author_522</th>\n",
       "      <th>item_author_523</th>\n",
       "      <th>item_author_524</th>\n",
       "      <th>item_avgTime</th>\n",
       "      <th>item_numReader</th>\n",
       "      <th>item_numTime</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013336</td>\n",
       "      <td>0.745461</td>\n",
       "      <td>0.069180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347415</th>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.009596</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369533</th>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071564</td>\n",
       "      <td>0.529903</td>\n",
       "      <td>0.263886</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239150</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215608</td>\n",
       "      <td>0.220345</td>\n",
       "      <td>0.330593</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59615</th>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.114120</td>\n",
       "      <td>0.143617</td>\n",
       "      <td>0.114050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_age  user_gender_0  user_gender_1  user_gender_2  user_gender_3  \\\n",
       "2496    0.400000            0.0            1.0            0.0            0.0   \n",
       "347415  0.293333            0.0            1.0            0.0            0.0   \n",
       "369533  0.346667            0.0            1.0            0.0            0.0   \n",
       "239150  0.360000            0.0            1.0            0.0            0.0   \n",
       "59615   0.213333            0.0            1.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_0  user_7_hero_1  user_7_hero_2  user_7_hero_3  \\\n",
       "2496              0.0            0.0            0.0            0.0   \n",
       "347415            0.0            0.0            0.0            0.0   \n",
       "369533            0.0            0.0            0.0            0.0   \n",
       "239150            0.0            0.0            0.0            0.0   \n",
       "59615             0.0            0.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_4  ...  item_author_519  item_author_520  item_author_521  \\\n",
       "2496              0.0  ...              0.0              0.0              0.0   \n",
       "347415            0.0  ...              0.0              0.0              0.0   \n",
       "369533            0.0  ...              0.0              0.0              0.0   \n",
       "239150            0.0  ...              0.0              0.0              0.0   \n",
       "59615             0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "        item_author_522  item_author_523  item_author_524  item_avgTime  \\\n",
       "2496                0.0              0.0              0.0      0.013336   \n",
       "347415              0.0              0.0              0.0      0.044509   \n",
       "369533              0.0              0.0              0.0      0.071564   \n",
       "239150              0.0              0.0              0.0      0.215608   \n",
       "59615               0.0              0.0              0.0      0.114120   \n",
       "\n",
       "        item_numReader  item_numTime  label  \n",
       "2496          0.745461      0.069180    0.0  \n",
       "347415        0.030982      0.009596    1.0  \n",
       "369533        0.529903      0.263886    1.0  \n",
       "239150        0.220345      0.330593    1.0  \n",
       "59615         0.143617      0.114050    0.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = [\"user_age\", \"user_gender\", \"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_rate\", \"item_keyword\", \"item_author\", \"item_avgTime\", \"item_numReader\", \"item_numTime\", \"label\"]\n",
    "raw = pd.read_csv(\"./thing.txt\", names=head, sep=\",\", index_col = False)\n",
    "\n",
    "colIdx = raw.columns.values.tolist().index(\"user_gender\")\n",
    "raw = genderDummy(raw, \"user_gender\", colIdx)\n",
    "colIdx = raw.columns.values.tolist().index(\"item_keyword\")\n",
    "raw = toDummy(raw, \"item_keyword\", colIdx)\n",
    "\n",
    "numDic = {\"user_gender\": 1, \"user_7_hero\": 5, \"user_30_hero\": 5, \"user_7_keyword\": 3, \"user_7_author\": 3, \"item_keyword\": 1, \"item_author\": 3}\n",
    "for i in [\"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_author\"]:\n",
    "    colIdx = raw.columns.values.tolist().index(i)\n",
    "    raw = sparseEmbed(raw, i, numDic[i], colIdx)\n",
    "    print(\"finished with\", i)\n",
    "\n",
    "# normalize numerical features into interval [0, 1]\n",
    "for i in [\"user_age\", \"item_rate\", \"item_avgTime\", \"item_numReader\", \"item_numTime\"]:\n",
    "    r = raw[i].values.astype(float)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    raw_normalized = pd.DataFrame(x_scaled)\n",
    "    raw[i] = raw_normalized\n",
    "\n",
    "raw = raw.sample(200000)\n",
    "    \n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: (3000, 2118) | validate data size: (1000, 2118) | testing data size: (1000, 2118)\n",
      "Epoch:   1 | Train Loss:  +319.58 | Val Loss: +319.566 | Train Neck:   +1.711 | Val Neck:   +1.722 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 07.382 | mean neg dist 07.397 \n",
      "Epoch:  51 | Train Loss:  +311.44 | Val Loss: +311.210 | Train Neck:   +1.711 | Val Neck:   +1.722 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 07.309 | mean neg dist 07.345 \n",
      "Epoch: 101 | Train Loss:  +217.68 | Val Loss: +215.478 | Train Neck:   +1.711 | Val Neck:   +1.722 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 07.093 | mean neg dist 07.096 \n",
      "Epoch: 151 | Train Loss:    +3.34 | Val Loss:   +2.989 | Train Neck:   +0.955 | Val Neck:   +0.962 | Train AUC:   +0.489 | Val AUC:   +0.513 | TF Break: 00.500 | mean pos dist: 04.627 | mean neg dist 04.628 \n",
      "Epoch: 201 | Train Loss:    +0.08 | Val Loss:   +0.140 | Train Neck:   +0.164 | Val Neck:   +0.276 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 00.703 | mean neg dist 00.703 \n",
      "Epoch: 251 | Train Loss:    +1.74 | Val Loss:   +1.812 | Train Neck:   +0.152 | Val Neck:   +0.133 | Train AUC:   +0.530 | Val AUC:   +0.490 | TF Break: 00.500 | mean pos dist: 00.235 | mean neg dist 00.236 \n",
      "Epoch: 301 | Train Loss:    +1.09 | Val Loss:   +0.939 | Train Neck:   +0.521 | Val Neck:   +0.218 | Train AUC:   +0.550 | Val AUC:   +0.501 | TF Break: 00.500 | mean pos dist: 00.959 | mean neg dist 01.066 \n",
      "Epoch: 351 | Train Loss:    +0.78 | Val Loss:   +0.804 | Train Neck:   +0.059 | Val Neck:   +0.096 | Train AUC:   +0.583 | Val AUC:   +0.562 | TF Break: 00.500 | mean pos dist: 00.114 | mean neg dist 00.114 \n",
      "Epoch: 401 | Train Loss:    +0.94 | Val Loss:   +0.811 | Train Neck:   +0.212 | Val Neck:   +0.140 | Train AUC:   +0.512 | Val AUC:   +0.511 | TF Break: 00.500 | mean pos dist: 00.227 | mean neg dist 00.227 \n",
      "Epoch: 451 | Train Loss:    +0.74 | Val Loss:   +0.705 | Train Neck:   +0.225 | Val Neck:   +0.095 | Train AUC:   +0.490 | Val AUC:   +0.484 | TF Break: 00.500 | mean pos dist: 00.208 | mean neg dist 00.208 \n",
      "Epoch: 501 | Train Loss:    +1.04 | Val Loss:   +1.019 | Train Neck:   +1.080 | Val Neck:   +1.063 | Train AUC:   +0.500 | Val AUC:   +0.573 | TF Break: 00.500 | mean pos dist: 05.129 | mean neg dist 05.130 \n",
      "Epoch: 551 | Train Loss:    +0.77 | Val Loss:   +0.755 | Train Neck:   +0.064 | Val Neck:   +0.056 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 00.080 | mean neg dist 00.080 \n",
      "Epoch: 601 | Train Loss:    +0.81 | Val Loss:   +0.742 | Train Neck:   +0.071 | Val Neck:   +0.066 | Train AUC:   +0.500 | Val AUC:   +0.500 | TF Break: 00.500 | mean pos dist: 00.183 | mean neg dist 00.183 \n",
      "Epoch: 651 | Train Loss:    +0.66 | Val Loss:   +0.731 | Train Neck:   +0.051 | Val Neck:   +0.295 | Train AUC:   +0.534 | Val AUC:   +0.530 | TF Break: 00.500 | mean pos dist: 00.163 | mean neg dist 00.163 \n",
      "Epoch: 701 | Train Loss:    +1.13 | Val Loss:   +0.826 | Train Neck:   +0.068 | Val Neck:   +0.072 | Train AUC:   +0.528 | Val AUC:   +0.477 | TF Break: 00.500 | mean pos dist: 00.234 | mean neg dist 00.234 \n",
      "Epoch: 751 | Train Loss:    +0.69 | Val Loss:   +0.735 | Train Neck:   +0.051 | Val Neck:   +0.193 | Train AUC:   +0.550 | Val AUC:   +0.465 | TF Break: 00.500 | mean pos dist: 00.113 | mean neg dist 00.113 \n",
      "Test Loss: 0.70114 | Neck Loss: 0.06801 | AUC: 0.50000\n"
     ]
    }
   ],
   "source": [
    "data = raw.sample(5000)\n",
    "\n",
    "# Splitting dataframe into train, validation, and testing\n",
    "dataY = data['label'].values\n",
    "dataX = data.drop(columns = 'label').values\n",
    "\n",
    "\n",
    "X, Xtest, Y, Ytest = train_test_split(dataX, dataY, test_size = 0.2, random_state = 42)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(\"training data size: {} | validate data size: {} | testing data size: {}\".format(str(Xtrain.shape), str(Xval.shape), str(Xtest.shape)))\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.5\n",
    "batch_size = 2048\n",
    "epochs = 800\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = Xtrain.shape[1]\n",
    "num_input_p = data.columns.values.tolist().index(\"item_rate\") # number of all user input columns, the last column ends before the start of \"item_rate\" column\n",
    "num_input_g = data.columns.values.shape[0] - num_input_p - 1   # number of all item input columns, = all column -user -label\n",
    "\n",
    "a = 8\n",
    "num_encode_1 = int(256 *a)\n",
    "num_encode_2 = int(128 *a)\n",
    "num_encode_3 = int(64 *a)\n",
    "num_encode_4 = int(32 *a)\n",
    "num_encode_5 = int(16*a)\n",
    "num_encode_6 = int(16*a)\n",
    "\n",
    "num_neck = 100\n",
    "\n",
    "num_decode_1 = num_encode_3\n",
    "num_decode_2 = num_encode_2\n",
    "num_decode_3 = num_encode_1\n",
    "\n",
    "num_output_to_p = num_input_p\n",
    "num_output_to_g = num_input_g\n",
    "\n",
    "#del raw\n",
    "\n",
    "\n",
    "# balance weight coefficient [0,1], the bigger the mode focused onto neck distance\n",
    "alpha = 0.3\n",
    "\n",
    "# regularization\n",
    "beta = 0.0005\n",
    "\n",
    "# collaborative autoencoder input tensor1\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "label = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "weights = {\n",
    "    'encoder_ph1': tf.Variable(tf.random_normal([num_input_p , num_encode_1])),\n",
    "    'encoder_gh1': tf.Variable(tf.random_normal([num_input_g , num_encode_1])),\n",
    "    'encoder_ph2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_gh2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_ph3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),\n",
    "    'encoder_gh3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),    \n",
    "    'encoder_ph4': tf.Variable(tf.random_normal([num_encode_3 , num_encode_4])),\n",
    "    'encoder_gh4': tf.Variable(tf.random_normal([num_encode_3 , num_encode_4])),    \n",
    "    'encoder_ph5': tf.Variable(tf.random_normal([num_encode_4 , num_encode_5])),\n",
    "    'encoder_gh5': tf.Variable(tf.random_normal([num_encode_4 , num_encode_5])),    \n",
    "    'encoder_ph6': tf.Variable(tf.random_normal([num_encode_5 , num_encode_6])),\n",
    "    'encoder_gh6': tf.Variable(tf.random_normal([num_encode_5 , num_encode_6])),        \n",
    "    \n",
    "    'encoder_pneck': tf.Variable(tf.random_normal([num_encode_6 , num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_gneck': tf.Variable(tf.random_normal([num_encode_6 , num_neck])), ## METRIC SPACE OF GOODS\n",
    "    \n",
    "    'decoder_gh1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_ph1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_gh2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    'decoder_ph2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    \n",
    "    \n",
    "    'decoder_gh3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),\n",
    "    'decoder_ph3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),    \n",
    "    \n",
    "    \n",
    "    'decoder_g_to_p_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_p])),\n",
    "    'decoder_p_to_g_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_g]))\n",
    "}\n",
    "\n",
    "biases = {  \n",
    "    'encoder_bph1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bgh1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bph2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bgh2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bph3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "    'encoder_bgh3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "    'encoder_bph4': tf.Variable(tf.random_normal([num_encode_4])),\n",
    "    'encoder_bgh4': tf.Variable(tf.random_normal([num_encode_4])),    \n",
    "    'encoder_bph5': tf.Variable(tf.random_normal([num_encode_5])),\n",
    "    'encoder_bgh5': tf.Variable(tf.random_normal([num_encode_5])),   \n",
    "    'encoder_bph6': tf.Variable(tf.random_normal([num_encode_6])),\n",
    "    'encoder_bgh6': tf.Variable(tf.random_normal([num_encode_6])),       \n",
    "    \n",
    "    'encoder_bpneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_bgneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF GOODS\n",
    "    'decoder_bgh1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bph1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bgh2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    'decoder_bph2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    \n",
    "    \n",
    "    'decoder_bgh3': tf.Variable(tf.random_normal([num_decode_3])),\n",
    "    'decoder_bph3': tf.Variable(tf.random_normal([num_decode_3])),    \n",
    "    \n",
    "    \n",
    "    'decoder_b_g_to_p_out': tf.Variable(tf.random_normal([num_output_to_p])),\n",
    "    'decoder_b_p_to_g_out': tf.Variable(tf.random_normal([num_output_to_g]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "\n",
    "    ## Person encoder:\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(x[:, :1376], weights['encoder_ph1']), biases['encoder_bph1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['encoder_ph2']), biases['encoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['encoder_ph3']), biases['encoder_bph3']))\n",
    "    layer_p_4 = tf.nn.relu(tf.add(tf.matmul(layer_p_3, weights['encoder_ph4']), biases['encoder_bph4']))\n",
    "    layer_p_5 = tf.nn.relu(tf.add(tf.matmul(layer_p_4, weights['encoder_ph5']), biases['encoder_bph5']))\n",
    "    layer_p_6 = tf.nn.relu(tf.add(tf.matmul(layer_p_5, weights['encoder_ph6']), biases['encoder_bph6']))\n",
    "    \n",
    "    layer_p_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_6, weights['encoder_pneck']), biases['encoder_bpneck']))\n",
    "    \n",
    "    ## Good encoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(x[:, 1376:], weights['encoder_gh1']), biases['encoder_bgh1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['encoder_gh2']), biases['encoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['encoder_gh3']), biases['encoder_bgh3']))   \n",
    "    layer_g_4 = tf.nn.relu(tf.add(tf.matmul(layer_g_3, weights['encoder_gh4']), biases['encoder_bgh4']))   \n",
    "    layer_g_5 = tf.nn.relu(tf.add(tf.matmul(layer_g_4, weights['encoder_gh5']), biases['encoder_bgh5']))   \n",
    "    layer_g_6 = tf.nn.relu(tf.add(tf.matmul(layer_g_5, weights['encoder_gh6']), biases['encoder_bgh6']))\n",
    "    \n",
    "    layer_g_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_6, weights['encoder_gneck']), biases['encoder_bgneck']))\n",
    "    \n",
    "    \n",
    "    return layer_p_neck, layer_g_neck\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(p_neck, g_neck):\n",
    "    \n",
    "    ## Good to Person decoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(g_neck, weights['decoder_gh1']), biases['decoder_bgh1']))\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['decoder_gh2']), biases['decoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['decoder_gh3']), biases['decoder_bgh3']))\n",
    "\n",
    "    layer_g_to_p_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_3, weights['decoder_g_to_p_out']), biases['decoder_b_g_to_p_out']))\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Person to Good decoder\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(p_neck, weights['decoder_ph1']), biases['decoder_bph1']))\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['decoder_ph2']), biases['decoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['decoder_ph3']), biases['decoder_bph3']))\n",
    "\n",
    "    layer_p_to_g_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_3, weights['decoder_p_to_g_out']), biases['decoder_b_p_to_g_out']))\n",
    "    \n",
    "    result = tf.concat([layer_g_to_p_out, layer_p_to_g_out], axis = 1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def getl2loss(dic):\n",
    "    l2 = 0\n",
    "    for i in dic.keys():\n",
    "        l2 += tf.nn.l2_loss(dic[i])\n",
    "    return l2\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_p, encoder_g = encoder(X)\n",
    "decoder_out = decoder(encoder_p, encoder_g)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_out\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "\n",
    "# calculate l2 distance\n",
    "neck_distance_l2 = tf.reshape(tf.norm(encoder_p-encoder_g, axis = 1), [-1,1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate l infinity distance\n",
    "neck_distance = tf.reshape(tf.norm(encoder_p - encoder_g, axis=1, ord=np.inf), [-1,1])\n",
    "\n",
    "signed_distance = tf.multiply(neck_distance, 2*label-1)\n",
    "negative_mask = tf.less_equal(signed_distance, 0) # Getting a boolean vector that have TRUE for negative entries\n",
    "positive_mask = tf.greater(signed_distance, 0)\n",
    "\n",
    "training = tf.Variable(False)\n",
    "pos_neg_break = tf.Variable(0)\n",
    "\n",
    "\n",
    "if training:\n",
    "    furthest_positive = tf.reduce_mean(tf.boolean_mask(signed_distance, positive_mask))\n",
    "    closest_negative = -tf.reduce_mean(tf.boolean_mask(signed_distance, negative_mask))\n",
    "    \n",
    "    threshold = (furthest_positive + closest_negative) / 2\n",
    "else:\n",
    "    # TESTING: Plug-in pre-determined connec\n",
    "    threshold = pos_neg_break\n",
    "\n",
    "    \n",
    "# Calculating AUC\n",
    "neck_pred = tf.less_equal(neck_distance, threshold) # neck distance based prediction on clicking\n",
    "\n",
    "\n",
    "# 4 different losses\n",
    "\n",
    "\n",
    "sign = 2*label-1\n",
    "loss_neck_distance = tf.reduce_mean(tf.maximum(0.0, 0.6*threshold+np.multiply(sign, signed_distance-threshold)))\n",
    "\n",
    "\n",
    "\n",
    "#signed_centered_distance = -(neck_distance - tf.reduce_mean(neck_distance))\n",
    "#loss_neck_distance = tf.losses.hinge_loss(label, signed_centered_distance)\n",
    "\n",
    "loss_pred_distance = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "loss_weights = tf.reduce_sum(getl2loss(weights))\n",
    "loss_bias = tf.reduce_sum(getl2loss(biases))\n",
    "\n",
    "loss = alpha * loss_neck_distance + (1-alpha) * loss_pred_distance + beta * (loss_weights + loss_bias)\n",
    "\n",
    "# Define Optimizer\n",
    "p_var_list = [weights[\"encoder_ph1\"], weights[\"encoder_ph2\"], weights[\"encoder_ph3\"], weights[\"encoder_ph4\"], weights[\"encoder_ph5\"], weights[\"encoder_ph6\"], weights[\"encoder_pneck\"], \n",
    "              weights[\"decoder_ph1\"], weights[\"decoder_ph2\"], weights[\"decoder_ph3\"], weights[\"decoder_p_to_g_out\"],\n",
    "              biases[\"encoder_bph1\"], biases[\"encoder_bph2\"], biases[\"encoder_bph3\"], biases[\"encoder_bph4\"], biases[\"encoder_bph5\"], biases[\"encoder_bph6\"], biases[\"encoder_bpneck\"], \n",
    "              biases[\"decoder_bph1\"], biases[\"decoder_bph2\"], biases[\"decoder_bph3\"], biases[\"decoder_b_p_to_g_out\"]]\n",
    "\n",
    "g_var_list = [weights[\"encoder_gh1\"], weights[\"encoder_gh2\"], weights[\"encoder_gh3\"], weights[\"encoder_gh4\"], weights[\"encoder_gh5\"], weights[\"encoder_gh6\"], weights[\"encoder_gneck\"], \n",
    "              weights[\"decoder_gh1\"], weights[\"decoder_gh2\"], weights[\"decoder_gh3\"], weights[\"decoder_g_to_p_out\"],\n",
    "              biases[\"encoder_bgh1\"], biases[\"encoder_bgh2\"], biases[\"encoder_bgh3\"], biases[\"encoder_bgh4\"], biases[\"encoder_bgh5\"], biases[\"encoder_bgh6\"], biases[\"encoder_bgneck\"], \n",
    "              biases[\"decoder_bgh1\"], biases[\"decoder_bgh2\"], biases[\"decoder_bgh3\"], biases[\"decoder_b_g_to_p_out\"]]\n",
    "\n",
    "optimizer_p = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list = p_var_list)\n",
    "optimizer_g = tf.train.RMSPropOptimizer(learning_rate).minimize(loss, var_list = g_var_list)\n",
    "\n",
    "\n",
    "\"\"\"gvs_p = optimizer_p.compute_gradients(loss, var_list = p_var_list)\n",
    "capped_gvs_p = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs_p]\n",
    "train_op_p = optimizer_p.apply_gradients(capped_gvs)\n",
    "\n",
    "gvs_g = optimizer_g.compute_gradients(loss, var_list = g_var_list)\n",
    "capped_gvs_g = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs_g]\n",
    "train_op_g = optimizer_g.apply_gradients(capped_gvs)\"\"\"\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    sess.run(local_init)\n",
    "    \n",
    "    \n",
    "    num_train_batches = int(Xtrain.shape[0] / batch_size)\n",
    "    Xtrain = np.array_split(Xtrain, num_train_batches)\n",
    "    Ytrain = np.array_split(Ytrain, num_train_batches)\n",
    "    \n",
    "    for i in range(len(Ytrain)):\n",
    "        Ytrain[i] = np.reshape(Ytrain[i], [-1,1])\n",
    "    Yval = np.reshape(Yval, [-1,1])\n",
    "\n",
    "    \n",
    "    # Training with validating\n",
    "    true_false_break = 0.5\n",
    "    \n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_train_cost = 0\n",
    "        avg_train_neck_cost = 0\n",
    "        avg_auc = 0\n",
    "        for batch in range(len(Xtrain)):\n",
    "\n",
    "            # optimize the person side\n",
    "            _, l, neck, max_pos_dist, min_neg_dist, pred_train, neck_dis = sess.run([optimizer_p, loss, loss_neck_distance, furthest_positive, closest_negative, neck_pred, neck_distance_l2],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck, threshold:true_false_break, furthest_positive:max_pos_dist, closest_negative:min_neg_dist, neck_pred:pred_train, neck_distance = neck_dis}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: Ytrain[batch], training: False, pos_neg_break: true_false_break}) # because it is training, it doesn't matter what's the value of pos_neg_break \n",
    "           # optimize the goods side\n",
    "            _, l, neck, max_pos_dist, min_neg_dist, pred_train, neck_dis = sess.run([optimizer_g, loss, loss_neck_distance, furthest_positive, closest_negative, neck_pred, neck_distance_l2],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck, threshold:true_false_break, furthest_positive:max_pos_dist, closest_negative:min_neg_dist, neck_pred:pred_train}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: Ytrain[batch], training: False, pos_neg_break: true_false_break}) # because it is training, it doesn't matter what's the value of pos_neg_break \n",
    "            #print(l)\n",
    "            \"\"\"print(Xtrain[batch][0])\n",
    "            print(Ytrain[batch][0])\"\"\"\n",
    "            \n",
    "            avg_train_cost += l\n",
    "            avg_train_neck_cost += neck\n",
    "            avg_auc += roc_auc_score(Ytrain[batch], pred_train) # adding training batch training auc score\n",
    "        \n",
    "        avg_train_cost /= num_train_batches\n",
    "        avg_train_neck_cost /= num_train_batches\n",
    "        avg_auc /= num_train_batches # averaging out training batch auc score\n",
    "        \n",
    "        \"\"\"print(Xval[0])\n",
    "        print(Yval[0])\"\"\"\n",
    "        \n",
    "        # Validate once an epoch ends\n",
    "        val_cost, val_neck_cost, pred_val, neck_dis_val = sess.run([loss, loss_neck_distance, neck_pred, neck_distance_l2], # {loss:val_cost, loss_neck_distance:val_neck_cost, neck_pred:pred_val}\n",
    "                                                    feed_dict={X: Xval, label: Yval, training: False, pos_neg_break: true_false_break})\n",
    "        val_auc = roc_auc_score(Yval, pred_val) # getting validation auc score\n",
    "        \n",
    "        \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            # look at the training metric space\n",
    "            signed = np.multiply(neck_dis.flatten(), 2*Ytrain[batch].flatten()-1)\n",
    "            pos = []\n",
    "            neg = []\n",
    "            for dist in signed:\n",
    "                if dist >= 0:\n",
    "                    pos.append(dist)\n",
    "                else:\n",
    "                    neg.append(dist)\n",
    "\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            neg = np.array(neg)\n",
    "\n",
    "            a = np.mean(pos)\n",
    "            b = np.mean(neg)\n",
    "            \n",
    "            \n",
    "            # look at the validation metric space\n",
    "            signed_val = np.multiply(neck_dis_val.flatten(), 2*Yval.flatten()-1)\n",
    "            pos_val = []\n",
    "            neg_val = []\n",
    "            for dist_val in signed_val:\n",
    "                if dist_val >= 0:\n",
    "                    pos_val.append(dist_val)\n",
    "                else:\n",
    "                    neg_val.append(dist_val)\n",
    "\n",
    "            pos_val = np.array(pos_val)\n",
    "            neg_val = np.array(neg_val)\n",
    "\n",
    "            a_val = np.mean(pos_val)\n",
    "            b_val = np.mean(neg_val)\n",
    "            \n",
    "            \n",
    "            if i >= 2000:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "      \n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos_val, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg_val, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "                \n",
    "            #true_false_break = math.sqrt(num_neck) / 2\n",
    "                        \n",
    "            print(\"Epoch: {:>3} | Train Loss: {:+8.2f} | Val Loss: {:+8.3f} | Train Neck: {:+8.3f} | Val Neck: {:+8.3f} | Train AUC: {:+8.3f} | Val AUC: {:+8.3f} | TF Break: {:06.3f} | mean pos dist: {:06.3f} | mean neg dist {:06.3f} \"\n",
    "                  .format( i + 1,        avg_train_cost, val_cost,     avg_train_neck_cost, val_neck_cost,   avg_auc,val_auc,   true_false_break, a, -b))\n",
    "            \n",
    "    Ytest = np.reshape(Ytest, [-1,1])\n",
    "\n",
    "        \n",
    "    # Testing\n",
    "    test_cost, test_neck_cost, pred_test = sess.run([loss, loss_neck_distance, neck_pred],  # {loss:test_cost, loss_neck_distance:test_neck_cost, neck_pred:pred_test}\n",
    "                                                   feed_dict={X: Xtest, label: Ytest, training: False, pos_neg_break: true_false_break})\n",
    "    test_auc = roc_auc_score(Ytest, pred_test)  # getting testing auc score\n",
    "    print(\"Test Loss: {:02.5f} | Neck Loss: {:02.5f} | AUC: {:02.5f}\".format(test_cost, test_neck_cost, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Compressed Sensing to hash high dimensional sparse binary feature group into numerical low dimensional representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sparse feature group begin and ending column indecies\n",
    "user_sparse_feat = [\"user_gender_0\", \"user_7_hero_0\", \"user_30_hero_0\", \"user_7_keyword_0\", \"user_7_author_0\", \"item_rate\"]\n",
    "item_sparse_feat = [\"item_keyword_0\", \"item_author_0\", \"item_avgTime\"] \n",
    "\n",
    "user_sparse_feat_beginEnd = []\n",
    "item_sparse_feat_beginEnd = []\n",
    "\n",
    "for i in range(len(user_sparse_feat)-1):\n",
    "    begin = raw.columns.values.tolist().index(user_sparse_feat[i])\n",
    "    end = raw.columns.values.tolist().index(user_sparse_feat[i+1])-1\n",
    "    user_sparse_feat_beginEnd.append((begin, end))\n",
    "    \n",
    "for i in range(len(item_sparse_feat)-1):\n",
    "    begin = raw.columns.values.tolist().index(item_sparse_feat[i])\n",
    "    end = raw.columns.values.tolist().index(item_sparse_feat[i+1])-1\n",
    "    item_sparse_feat_beginEnd.append((begin, end))\n",
    "    \n",
    "print(\"user sparse feature begin end index:\", str(user_sparse_feat_beginEnd))\n",
    "print(\"item sparse feature begin end index:\", str(item_sparse_feat_beginEnd))\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _weights(x, dx=1, orig=0):\n",
    "    x = np.ravel(x)\n",
    "    floor_x = np.floor((x - orig) / dx).astype(np.int64)\n",
    "    alpha = (x - orig - floor_x * dx) / dx\n",
    "    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n",
    "\n",
    "\n",
    "def _generate_center_coordinates(l_x):\n",
    "    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n",
    "    center = l_x / 2.\n",
    "    X += 0.5 - center\n",
    "    Y += 0.5 - center\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def build_projection_operator(l_x, n_dir):\n",
    "    \"\"\" Compute the tomography design matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    l_x : int\n",
    "        linear size of image array\n",
    "\n",
    "    n_dir : int\n",
    "        number of angles at which projections are acquired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : sparse matrix of shape (n_dir l_x, l_x**2)\n",
    "    \"\"\"\n",
    "    X, Y = _generate_center_coordinates(l_x)\n",
    "    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n",
    "    data_inds, weights, camera_inds = [], [], []\n",
    "    data_unravel_indices = np.arange(l_x ** 2)\n",
    "    data_unravel_indices = np.hstack((data_unravel_indices,\n",
    "                                      data_unravel_indices))\n",
    "    for i, angle in enumerate(angles):\n",
    "        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n",
    "        inds, w = _weights(Xrot, dx=1, orig=X.min())\n",
    "        mask = np.logical_and(inds >= 0, inds < l_x)\n",
    "        weights += list(w[mask])\n",
    "        camera_inds += list(inds[mask] + i * l_x)\n",
    "        data_inds += list(data_unravel_indices[mask])\n",
    "    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n",
    "    return proj_operator\n",
    "\n",
    "\n",
    "def generate_synthetic_data():\n",
    "    \"\"\" Synthetic binary data \"\"\"\n",
    "    rs = np.random.RandomState(0)\n",
    "    n_pts = 36\n",
    "    x, y = np.ogrid[0:l, 0:l]\n",
    "    mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 < (l / 2.) ** 2\n",
    "    mask = np.zeros((l, l))\n",
    "    points = l * rs.rand(2, n_pts)\n",
    "    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
    "    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n",
    "    res = np.logical_and(mask > mask.mean(), mask_outer)\n",
    "    return np.logical_xor(res, ndimage.binary_erosion(res))\n",
    "\n",
    "\n",
    "# Generate synthetic images, and projections\n",
    "l = 128\n",
    "proj_operator = build_projection_operator(l, l // 2)\n",
    "data = generate_synthetic_data()\n",
    "\n",
    "\n",
    "proj = proj_operator * data.ravel()[:, np.newaxis]\n",
    "proj += 0.15 * np.random.randn(*proj.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Reconstruction with L1 (Lasso) penalization\n",
    "# the best value of alpha was determined using cross validation\n",
    "# with LassoCV\n",
    "rgr_lasso = Lasso(alpha=0.001)\n",
    "rgr_lasso.fit(proj_operator, proj.ravel())\n",
    "rec_l1 = rgr_lasso.coef_.reshape(l, l)\n",
    "\n",
    "plt.figure(figsize=(8, 3.3))\n",
    "plt.subplot(131)\n",
    "plt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.title('original image')\n",
    "plt.subplot(132)\n",
    "\n",
    "plt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\n",
    "plt.title('L1 penalization')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n",
    "                    right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.drop(columns='label', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuck_true = np.array([0, 0, 1, 1])\n",
    "fuck_scores = np.array([False, False, False, True])\n",
    "roc_auc_score(fuck_true, fuck_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import seaborn as sns\n",
    "\n",
    "iris = load_iris()\n",
    "iris = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                    columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "# Sort the dataframe by target\n",
    "target_0 = iris.loc[iris['target'] == 0]\n",
    "target_1 = iris.loc[iris['target'] == 1]\n",
    "target_2 = iris.loc[iris['target'] == 2]\n",
    "\n",
    "sns.distplot(pos, hist=False, kde_kws={\"shade\": True})\n",
    "sns.distplot(neg, hist=False, kde_kws={\"shade\": True})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.446"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  1.78813920e-07,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.78813920e-07, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  8.94069601e-08,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  2.38418565e-07, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        1.19209282e-07, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  2.98023206e-08,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        6.66400197e-08,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  4.37231665e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        8.94069601e-08,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        2.98023206e-08, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  1.19209282e-07,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  8.81850647e-05,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  2.40273977e-07, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
