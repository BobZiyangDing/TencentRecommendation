{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-Collaborative Filtering Autoencoder Metric Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def sparseEmbed(df, name, num, colIdx):\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)] \n",
    "    Emptydf = pd.DataFrame()\n",
    "    Emptydf[embedName] = df[name].str.split('|',expand=True)\n",
    "    values = np.unique(Emptydf[embedName].values)\n",
    "    \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in values:\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "    \n",
    "    \n",
    "    appendValue = np.zeros([Emptydf.values.shape[0], len(values)])\n",
    "    for i in range(Emptydf.values.shape[0]):\n",
    "        for j in range(num):\n",
    "            key = Emptydf.values[i][j]\n",
    "            if key in dic:\n",
    "                appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def toDummy(df, name, colIdx):\n",
    "    num = len(np.unique(df[name].values.astype(str)))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df\n",
    "\n",
    "def genderDummy(df, name, colIdx):\n",
    "    pool = set()\n",
    "    num = len(np.unique(df[name].values))-1\n",
    "    for i in df[name].values:\n",
    "        pool.add(str(i))\n",
    "    num = len(list(pool))-1\n",
    "    embedName = [ name+\"_\"+str(i) for i in range(num)]  # don't need nan value\n",
    "        \n",
    "    dic = {}\n",
    "    a = 0\n",
    "    for i in range(num+1):\n",
    "        dic[i] = a\n",
    "        a += 1\n",
    "    dic.pop('nan', None)\n",
    "        \n",
    "    appendValue = np.zeros([df[name].size, a])\n",
    "    for i in range(df[name].size):\n",
    "        key = df[name].values[i]\n",
    "        if key in dic:\n",
    "            appendValue[i][dic[key]] = 1\n",
    "    \n",
    "    for i in range(appendValue.shape[1], 0, -1):\n",
    "        df.insert(colIdx, name+\"_\"+str(i-1), appendValue[:, i-1])\n",
    "    \n",
    "    del df[name]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and transforming to categorical binary input data form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with user_7_hero\n",
      "finished with user_30_hero\n",
      "finished with user_7_keyword\n",
      "finished with user_7_author\n",
      "finished with item_author\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_gender_0</th>\n",
       "      <th>user_gender_1</th>\n",
       "      <th>user_gender_2</th>\n",
       "      <th>user_gender_3</th>\n",
       "      <th>user_7_hero_0</th>\n",
       "      <th>user_7_hero_1</th>\n",
       "      <th>user_7_hero_2</th>\n",
       "      <th>user_7_hero_3</th>\n",
       "      <th>user_7_hero_4</th>\n",
       "      <th>...</th>\n",
       "      <th>item_author_519</th>\n",
       "      <th>item_author_520</th>\n",
       "      <th>item_author_521</th>\n",
       "      <th>item_author_522</th>\n",
       "      <th>item_author_523</th>\n",
       "      <th>item_author_524</th>\n",
       "      <th>item_avgTime</th>\n",
       "      <th>item_numReader</th>\n",
       "      <th>item_numTime</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>310480</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098042</td>\n",
       "      <td>0.084285</td>\n",
       "      <td>0.057503</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134963</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076348</td>\n",
       "      <td>0.193156</td>\n",
       "      <td>0.102620</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200994</th>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.203674</td>\n",
       "      <td>0.074629</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25616</th>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045387</td>\n",
       "      <td>0.585855</td>\n",
       "      <td>0.185033</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195170</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088159</td>\n",
       "      <td>0.031902</td>\n",
       "      <td>0.019571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_age  user_gender_0  user_gender_1  user_gender_2  user_gender_3  \\\n",
       "310480  0.320000            0.0            1.0            0.0            0.0   \n",
       "134963  0.266667            0.0            1.0            0.0            0.0   \n",
       "200994  0.293333            0.0            1.0            0.0            0.0   \n",
       "25616   0.280000            0.0            0.0            1.0            0.0   \n",
       "195170  0.360000            0.0            1.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_0  user_7_hero_1  user_7_hero_2  user_7_hero_3  \\\n",
       "310480            0.0            0.0            0.0            0.0   \n",
       "134963            0.0            0.0            0.0            0.0   \n",
       "200994            0.0            0.0            0.0            0.0   \n",
       "25616             0.0            0.0            0.0            0.0   \n",
       "195170            0.0            0.0            0.0            0.0   \n",
       "\n",
       "        user_7_hero_4  ...  item_author_519  item_author_520  item_author_521  \\\n",
       "310480            0.0  ...              0.0              0.0              0.0   \n",
       "134963            0.0  ...              0.0              0.0              0.0   \n",
       "200994            0.0  ...              0.0              0.0              0.0   \n",
       "25616             0.0  ...              0.0              0.0              0.0   \n",
       "195170            0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "        item_author_522  item_author_523  item_author_524  item_avgTime  \\\n",
       "310480              0.0              0.0              0.0      0.098042   \n",
       "134963              0.0              0.0              0.0      0.076348   \n",
       "200994              0.0              0.0              0.0      0.052656   \n",
       "25616               0.0              0.0              0.0      0.045387   \n",
       "195170              0.0              0.0              0.0      0.088159   \n",
       "\n",
       "        item_numReader  item_numTime  label  \n",
       "310480        0.084285      0.057503    1.0  \n",
       "134963        0.193156      0.102620    1.0  \n",
       "200994        0.203674      0.074629    1.0  \n",
       "25616         0.585855      0.185033    0.0  \n",
       "195170        0.031902      0.019571    1.0  \n",
       "\n",
       "[5 rows x 2119 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = [\"user_age\", \"user_gender\", \"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_rate\", \"item_keyword\", \"item_author\", \"item_avgTime\", \"item_numReader\", \"item_numTime\", \"label\"]\n",
    "raw = pd.read_csv(\"./thing.txt\", names=head, sep=\",\", index_col = False)\n",
    "\n",
    "colIdx = raw.columns.values.tolist().index(\"user_gender\")\n",
    "raw = genderDummy(raw, \"user_gender\", colIdx)\n",
    "colIdx = raw.columns.values.tolist().index(\"item_keyword\")\n",
    "raw = toDummy(raw, \"item_keyword\", colIdx)\n",
    "\n",
    "numDic = {\"user_gender\": 1, \"user_7_hero\": 5, \"user_30_hero\": 5, \"user_7_keyword\": 3, \"user_7_author\": 3, \"item_keyword\": 1, \"item_author\": 3}\n",
    "for i in [\"user_7_hero\", \"user_30_hero\", \"user_7_keyword\", \"user_7_author\", \"item_author\"]:\n",
    "    colIdx = raw.columns.values.tolist().index(i)\n",
    "    raw = sparseEmbed(raw, i, numDic[i], colIdx)\n",
    "    print(\"finished with\", i)\n",
    "\n",
    "# normalize numerical features into interval [0, 1]\n",
    "for i in [\"user_age\", \"item_rate\", \"item_avgTime\", \"item_numReader\", \"item_numTime\"]:\n",
    "    r = raw[i].values.astype(float)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(r.reshape(-1,1))\n",
    "    raw_normalized = pd.DataFrame(x_scaled)\n",
    "    raw[i] = raw_normalized\n",
    "\n",
    "raw = raw.sample(200000)\n",
    "    \n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: (3000, 2118) | validate data size: (1000, 2118) | testing data size: (1000, 2118)\n",
      "48.164127 2.8375542 0.48164126\n",
      "48.395462 2.8963997 0.48395464\n",
      "47.529938 2.7160268 0.47529936\n",
      "47.54924 2.7840497 0.47549242\n",
      "46.9064 2.7830703 0.469064\n",
      "46.950104 2.971501 0.46950102\n",
      "46.845654 3.0212436 0.46845654\n",
      "46.437317 3.101847 0.46437317\n",
      "45.950882 2.7713938 0.4595088\n",
      "45.417267 2.8573728 0.45417267\n",
      "45.564926 2.6847625 0.45564926\n",
      "0.46882846680554474\n",
      "Epoch:   1 | Train Loss:   +46.88 | Val Loss:  +45.549 | Train Neck:   +2.857 | Val Neck:   +0.000 | Train Recon:   +0.469 | Val Recon:   +0.455\n",
      "---------- | Train AUC:   +0.000 | Val AUC:   +0.000 | TF Break: 01.581 | mean pos dist: 01.828 | mean neg dist 01.846 \n",
      "\n",
      "45.294685 2.9233248 0.45294684\n",
      "45.359333 2.896703 0.4535933\n",
      "44.94151 2.6915948 0.4494151\n",
      "44.502235 2.7385497 0.44502234\n",
      "44.242527 2.7661424 0.44242528\n",
      "44.468174 2.9126425 0.44468173\n",
      "44.436066 2.9136782 0.44436067\n",
      "44.266655 2.963677 0.44266653\n",
      "44.06622 2.7382412 0.4406622\n",
      "44.106304 2.815824 0.44106305\n",
      "43.9099 2.6590075 0.439099\n",
      "0.44508509744297375\n",
      "43.03512 2.9210973 0.4303512\n",
      "42.160107 2.8613381 0.42160106\n",
      "41.99676 2.6709182 0.41996762\n",
      "42.08585 2.712261 0.4208585\n",
      "42.676407 2.75965 0.42676407\n",
      "42.70984 2.8956354 0.4270984\n",
      "42.550606 2.9047585 0.42550606\n",
      "42.43559 2.9420998 0.4243559\n",
      "42.579254 2.7562888 0.42579255\n",
      "42.302982 2.8205576 0.4230298\n",
      "42.19353 2.6686044 0.42193532\n",
      "0.4242964061823758\n",
      "42.32314 2.8924582 0.4232314\n",
      "42.7358 2.8669946 0.427358\n",
      "42.872536 2.6949837 0.42872536\n",
      "42.704773 2.733577 0.42704773\n",
      "43.26593 2.7561834 0.4326593\n",
      "43.46733 2.9107535 0.4346733\n",
      "43.470417 2.931659 0.43470415\n",
      "43.45334 2.9670649 0.4345334\n",
      "43.33597 2.7529173 0.4333597\n",
      "43.15996 2.8213434 0.43159962\n",
      "42.905148 2.6664383 0.42905146\n",
      "0.43063122034072876\n",
      "42.756638 2.8902416 0.42756638\n",
      "42.69723 2.866995 0.4269723\n",
      "42.842735 2.701727 0.42842737\n",
      "42.75063 2.735606 0.4275063\n",
      "42.63927 2.7659385 0.4263927\n",
      "42.8932 2.940439 0.42893198\n",
      "42.433445 2.9549425 0.42433444\n",
      "42.12184 2.9917254 0.4212184\n",
      "42.00969 2.7554102 0.4200969\n",
      "42.068134 2.8629863 0.42068136\n",
      "42.23635 2.6823914 0.42236352\n",
      "0.4249537858096036\n",
      "42.218468 2.93101 0.42218468\n",
      "42.12705 2.9018369 0.4212705\n",
      "41.597702 2.709552 0.415977\n",
      "41.897217 2.7544606 0.41897216\n",
      "41.5479 2.7759962 0.415479\n",
      "41.48634 2.9779391 0.41486338\n",
      "41.377754 2.9830365 0.41377753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-8edabe47ed8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    261\u001b[0m             _, l, neck_loss, pred_loss, neck_dis_train, train_auc = sess.run([optimizer_g, loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n\u001b[0;32m    262\u001b[0m                                                         \u001b[1;31m#{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                                                         feed_dict={X: Xtrain[batch], label: np.reshape(Ytrain[batch], [-1]), threshold: true_false_break})\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mavg_train_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "data = raw.sample(5000)\n",
    "\n",
    "# Splitting dataframe into train, validation, and testing\n",
    "dataY = data['label'].values\n",
    "dataX = data.drop(columns = 'label').values\n",
    "\n",
    "\n",
    "X, Xtest, Y, Ytest = train_test_split(dataX, dataY, test_size = 0.2, random_state = 42)\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(\"training data size: {} | validate data size: {} | testing data size: {}\".format(str(Xtrain.shape), str(Xval.shape), str(Xtest.shape)))\n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 256\n",
    "epochs = 800\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = Xtrain.shape[1]\n",
    "num_input_p = data.columns.values.tolist().index(\"item_rate\") # number of all user input columns, the last column ends before the start of \"item_rate\" column\n",
    "num_input_g = data.columns.values.shape[0] - num_input_p - 1   # number of all item input columns, = all column -user -label\n",
    "\n",
    "a = 1\n",
    "num_encode_1 = int(256 *a)\n",
    "num_encode_2 = int(128 *a)\n",
    "num_encode_3 = int(64 *a)\n",
    "\n",
    "\n",
    "num_neck = 10\n",
    "\n",
    "num_decode_1 = num_encode_3\n",
    "num_decode_2 = num_encode_2\n",
    "num_decode_3 = num_encode_1\n",
    "\n",
    "num_output_to_p = num_input_p\n",
    "num_output_to_g = num_input_g\n",
    "\n",
    "#del raw\n",
    "\n",
    "\n",
    "# balance weight coefficient [0,1], the bigger the mode focused onto neck distance\n",
    "alpha = 0\n",
    "\n",
    "# regularization\n",
    "beta = 0\n",
    "\n",
    "# collaborative autoencoder input tensor1\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "label = tf.placeholder(\"float\", [None])\n",
    "threshold = tf.Variable(0.0)\n",
    "\n",
    "weights = {\n",
    "    'encoder_ph1': tf.Variable(tf.random_normal([num_input_p , num_encode_1])),\n",
    "    'encoder_gh1': tf.Variable(tf.random_normal([num_input_g , num_encode_1])),\n",
    "    'encoder_ph2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_gh2': tf.Variable(tf.random_normal([num_encode_1 , num_encode_2])),\n",
    "    'encoder_ph3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),\n",
    "    'encoder_gh3': tf.Variable(tf.random_normal([num_encode_2 , num_encode_3])),    \n",
    "\n",
    "\n",
    "    'encoder_pneck': tf.Variable(tf.random_normal([num_encode_3 , num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_gneck': tf.Variable(tf.random_normal([num_encode_3 , num_neck])), ## METRIC SPACE OF GOODS\n",
    "\n",
    "\n",
    "    'decoder_gh1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_ph1': tf.Variable(tf.random_normal([num_neck , num_decode_1])),\n",
    "    'decoder_gh2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    'decoder_ph2': tf.Variable(tf.random_normal([num_decode_1 , num_decode_2])),\n",
    "    'decoder_gh3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),\n",
    "    'decoder_ph3': tf.Variable(tf.random_normal([num_decode_2 , num_decode_3])),    \n",
    "\n",
    "\n",
    "    'decoder_g_to_p_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_p])),\n",
    "    'decoder_p_to_g_out': tf.Variable(tf.random_normal([num_decode_3 , num_output_to_g]))\n",
    "}\n",
    "\n",
    "biases = {  \n",
    "    'encoder_bph1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bgh1': tf.Variable(tf.random_normal([num_encode_1])),\n",
    "    'encoder_bph2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bgh2': tf.Variable(tf.random_normal([num_encode_2])),\n",
    "    'encoder_bph3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "    'encoder_bgh3': tf.Variable(tf.random_normal([num_encode_3])),\n",
    "\n",
    "\n",
    "    'encoder_bpneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF PERSON\n",
    "    'encoder_bgneck': tf.Variable(tf.random_normal([num_neck])), ## METRIC SPACE OF GOODS\n",
    "\n",
    "\n",
    "    'decoder_bgh1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bph1': tf.Variable(tf.random_normal([num_decode_1])),\n",
    "    'decoder_bgh2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    'decoder_bph2': tf.Variable(tf.random_normal([num_decode_2])),\n",
    "    'decoder_bgh3': tf.Variable(tf.random_normal([num_decode_3])),\n",
    "    'decoder_bph3': tf.Variable(tf.random_normal([num_decode_3])),    \n",
    "\n",
    "\n",
    "    'decoder_b_g_to_p_out': tf.Variable(tf.random_normal([num_output_to_p])),\n",
    "    'decoder_b_p_to_g_out': tf.Variable(tf.random_normal([num_output_to_g]))\n",
    "}\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "\n",
    "    ## Person encoder:\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(x[:, :1376], weights['encoder_ph1']), biases['encoder_bph1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['encoder_ph2']), biases['encoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['encoder_ph3']), biases['encoder_bph3']))\n",
    "\n",
    "    layer_p_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_3, weights['encoder_pneck']), biases['encoder_bpneck']))\n",
    "\n",
    "    ## Good encoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(x[:, 1376:], weights['encoder_gh1']), biases['encoder_bgh1']))  ## HARD CODING: 1375 is the ending index of person feature; 1376 the starting index of goods feature\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['encoder_gh2']), biases['encoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['encoder_gh3']), biases['encoder_bgh3']))\n",
    "\n",
    "    layer_g_neck = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_3, weights['encoder_gneck']), biases['encoder_bgneck']))\n",
    "\n",
    "\n",
    "    return layer_p_neck, layer_g_neck\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(p_neck, g_neck):\n",
    "\n",
    "    ## Good to Person decoder\n",
    "    layer_g_1 = tf.nn.relu(tf.add(tf.matmul(g_neck, weights['decoder_gh1']), biases['decoder_bgh1']))\n",
    "    layer_g_2 = tf.nn.relu(tf.add(tf.matmul(layer_g_1, weights['decoder_gh2']), biases['decoder_bgh2']))\n",
    "    layer_g_3 = tf.nn.relu(tf.add(tf.matmul(layer_g_2, weights['decoder_gh3']), biases['decoder_bgh3']))\n",
    "\n",
    "    layer_g_to_p_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_g_3, weights['decoder_g_to_p_out']), biases['decoder_b_g_to_p_out']))\n",
    "\n",
    "\n",
    "\n",
    "    ## Person to Good decoder\n",
    "    layer_p_1 = tf.nn.relu(tf.add(tf.matmul(p_neck, weights['decoder_ph1']), biases['decoder_bph1']))\n",
    "    layer_p_2 = tf.nn.relu(tf.add(tf.matmul(layer_p_1, weights['decoder_ph2']), biases['decoder_bph2']))\n",
    "    layer_p_3 = tf.nn.relu(tf.add(tf.matmul(layer_p_2, weights['decoder_ph3']), biases['decoder_bph3']))\n",
    "\n",
    "    layer_p_to_g_out = tf.nn.sigmoid(tf.add(tf.matmul(layer_p_3, weights['decoder_p_to_g_out']), biases['decoder_b_p_to_g_out']))\n",
    "\n",
    "    result = tf.concat([layer_g_to_p_out, layer_p_to_g_out], axis = 1)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Construct model\n",
    "\n",
    "encoder_p, encoder_g = encoder(X)\n",
    "decoder_out = decoder(encoder_p, encoder_g)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_out\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "def getl2loss(dic):\n",
    "    l2 = 0\n",
    "    for i in dic.keys():\n",
    "        l2 += tf.nn.l2_loss(dic[i])\n",
    "    return l2\n",
    "\n",
    "sign = 2*label-1\n",
    "\n",
    "\n",
    "# calculate l2 distance\n",
    "neck_distance_l2 = tf.reshape(tf.norm(encoder_p-encoder_g, axis = 1), [-1])\n",
    "signed_distance_l2 = tf.multiply(neck_distance_l2, sign)\n",
    "\n",
    "\n",
    "\n",
    "# calculate l infinity distance\n",
    "# neck_distance_linf = tf.reshape(tf.norm(encoder_p - encoder_g, axis=1, ord = np.infty), [-1])\n",
    "# signed_distance_linf = tf.multiply(neck_distance_linf, sign)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4 different losses\n",
    "#loss_neck_distance = tf.reduce_mean(tf.maximum(0.0, 0.6*threshold+tf.multiply(sign, signed_distance_l2-threshold)))\n",
    "\n",
    "\n",
    "\n",
    "#signed_centered_distance = -(neck_distance - tf.reduce_mean(neck_distance))\n",
    "loss_neck_distance = tf.losses.hinge_loss(label, tf.multiply(sign, signed_distance_l2-threshold))\n",
    "loss_pred_distance = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "loss_weights = tf.reduce_sum(getl2loss(weights))\n",
    "loss_bias = tf.reduce_sum(getl2loss(biases))\n",
    "\n",
    "loss = alpha * loss_neck_distance + (100-alpha) * loss_pred_distance + beta * (loss_weights + loss_bias)\n",
    "\n",
    "\n",
    "# Calculating AUC: \n",
    "# NOTE: because threshold is always half of maximum distance, \n",
    "# 2*threshold is maximum distance, normalize to scale of 1\n",
    "# then use 1 to minus will yield a inversion that matches distance proximity property\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auc, _ = tf.metrics.accuracy(label, tf.less_equal(threshold, neck_distance_l2))\n",
    "\n",
    "\n",
    "# Define Optimizer\n",
    "p_var_list = [weights[\"encoder_ph1\"], weights[\"encoder_ph2\"], weights[\"encoder_ph3\"], weights[\"encoder_pneck\"], \n",
    "              weights[\"decoder_ph1\"], weights[\"decoder_ph2\"], weights[\"decoder_ph3\"], weights[\"decoder_p_to_g_out\"],\n",
    "              biases[\"encoder_bph1\"], biases[\"encoder_bph2\"], biases[\"encoder_bph3\"], biases[\"encoder_bpneck\"], \n",
    "              biases[\"decoder_bph1\"], biases[\"decoder_bph2\"], biases[\"decoder_bph3\"], biases[\"decoder_b_p_to_g_out\"]]\n",
    "\n",
    "g_var_list = [weights[\"encoder_gh1\"], weights[\"encoder_gh2\"], weights[\"encoder_gh3\"], weights[\"encoder_gneck\"], \n",
    "              weights[\"decoder_gh1\"], weights[\"decoder_gh2\"], weights[\"decoder_gh3\"], weights[\"decoder_g_to_p_out\"],\n",
    "              biases[\"encoder_bgh1\"], biases[\"encoder_bgh2\"], biases[\"encoder_bgh3\"], biases[\"encoder_bgneck\"], \n",
    "              biases[\"decoder_bgh1\"], biases[\"decoder_bgh2\"], biases[\"decoder_bgh3\"], biases[\"decoder_b_g_to_p_out\"]]\n",
    "\n",
    "optimizer_p = tf.train.AdagradOptimizer(learning_rate).minimize(loss, var_list = p_var_list)\n",
    "optimizer_g = tf.train.AdagradOptimizer(learning_rate).minimize(loss, var_list = g_var_list)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    sess.run(local_init)\n",
    "\n",
    "\n",
    "    num_train_batches = int(Xtrain.shape[0] / batch_size)\n",
    "    Xtrain = np.array_split(Xtrain, num_train_batches)\n",
    "    Ytrain = np.array_split(Ytrain, num_train_batches)\n",
    "\n",
    "    for i in range(len(Ytrain)):\n",
    "        Ytrain[i] = np.reshape(Ytrain[i], [-1,1])\n",
    "    Yval = np.reshape(Yval, [-1,1])\n",
    "\n",
    "\n",
    "\n",
    "    # Training with validating\n",
    "    true_false_break = math.sqrt(num_neck)/2\n",
    "\n",
    "    for i in range(epochs):\n",
    "\n",
    "        avg_train_cost = 0\n",
    "        avg_train_neck_cost = 0\n",
    "        avg_train_pred_cost = 0\n",
    "        avg_train_auc =0\n",
    "        for batch in range(len(Xtrain)):\n",
    "\n",
    "            # optimize the person side\n",
    "            _, l, neck_loss, pred_loss, neck_dis_train, train_auc = sess.run([optimizer_p, loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: np.reshape(Ytrain[batch], [-1]), threshold: true_false_break})\n",
    "           # optimize the goods side\n",
    "            _, l, neck_loss, pred_loss, neck_dis_train, train_auc = sess.run([optimizer_g, loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xtrain[batch], label: np.reshape(Ytrain[batch], [-1]), threshold: true_false_break})\n",
    "\n",
    "            avg_train_cost += l\n",
    "            avg_train_neck_cost += neck_loss\n",
    "            avg_train_pred_cost += pred_loss\n",
    "            avg_train_auc += train_auc\n",
    "            print(l, neck_loss, pred_loss)\n",
    "\n",
    "            \n",
    "        avg_train_cost /= num_train_batches\n",
    "        avg_train_neck_cost /= num_train_batches\n",
    "        avg_train_pred_cost /= num_train_batches\n",
    "        avg_train_auc /= num_train_batches \n",
    "            \n",
    "        print(avg_train_pred_cost)\n",
    "\n",
    "        # Validate once an epoch ends\n",
    "        val_cost, neck_loss, val_pred_loss, neck_dis_val, val_auc = sess.run([loss, loss_neck_distance, loss_pred_distance, neck_distance_l2, auc],  \n",
    "                                                        #{optimizer:_, loss:l, loss_neck_distance:neck_loss, loss_pred_distance:pred_loss, neck_distance_l2:neck_dis}\n",
    "                                                        feed_dict={X: Xval, label: np.reshape(Yval, [-1]), threshold: true_false_break})\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            # look at the training metric space\n",
    "            signed_train = np.multiply(neck_dis_train, 2*Ytrain[batch].flatten()-1)\n",
    "            pos = []\n",
    "            neg = []\n",
    "            for dist in signed_train:\n",
    "                if dist >= 0:\n",
    "                    pos.append(dist)\n",
    "                else:\n",
    "                    neg.append(dist)\n",
    "\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            neg = np.array(neg)\n",
    "\n",
    "            a = np.mean(pos)\n",
    "            b = np.mean(neg)\n",
    "\n",
    "\n",
    "            # look at the validation metric space\n",
    "            signed_val = np.multiply(neck_dis_val.flatten(), 2*Yval.flatten()-1)\n",
    "            pos_val = []\n",
    "            neg_val = []\n",
    "            for dist_val in signed_val:\n",
    "                if dist_val >= 0:\n",
    "                    pos_val.append(dist_val)\n",
    "                else:\n",
    "                    neg_val.append(dist_val)\n",
    "\n",
    "            pos_val = np.array(pos_val)\n",
    "            neg_val = np.array(neg_val)\n",
    "\n",
    "            a_val = np.mean(pos_val)\n",
    "            b_val = np.mean(neg_val)\n",
    "\n",
    "\n",
    "            if i >= 2000:\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                sns.set(color_codes=True)\n",
    "                sns.distplot(pos_val, bins=20, kde = False, color=\"r\", label=\"red: pos dis\")\n",
    "                sns.distplot(-neg_val, bins=20, kde = False , color=\"b\", label=\"blue: neg dis\")\n",
    "                plt.legend()\n",
    "                plt.xlim(0, 1)\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "            print(\"Epoch: {:>3} | Train Loss: {:+8.2f} | Val Loss: {:+8.3f} | Train Neck: {:+8.3f} | Val Neck: {:+8.3f} | Train Recon: {:+8.3f} | Val Recon: {:+8.3f}\"\n",
    "                  .format( i + 1,    avg_train_cost,         val_cost,        avg_train_neck_cost,      val_neck_cost,       avg_train_pred_cost,    val_pred_loss))\n",
    "            print(\"---------- | Train AUC: {:+8.3f} | Val AUC: {:+8.3f} | TF Break: {:06.3f} | mean pos dist: {:06.3f} | mean neg dist {:06.3f} \"\n",
    "                  .format(avg_auc,val_auc,   true_false_break, a, -b))\n",
    "            print()\n",
    "\n",
    "    Ytest = np.reshape(Ytest, [-1,1])\n",
    "\n",
    "\n",
    "    # Testing\n",
    "    test_cost, test_neck_cost, pred_test = sess.run([loss, loss_neck_distance, neck_pred],  # {loss:test_cost, loss_neck_distance:test_neck_cost, neck_pred:pred_test}\n",
    "                                                   feed_dict={X: Xtest, label: Ytest, training: False, pos_neg_break: true_false_break})\n",
    "    test_auc = roc_auc_score(Ytest, pred_test)  # getting testing auc score\n",
    "    print(\"Test Loss: {:02.5f} | Neck Loss: {:02.5f} | AUC: {:02.5f}\".format(test_cost, test_neck_cost, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
